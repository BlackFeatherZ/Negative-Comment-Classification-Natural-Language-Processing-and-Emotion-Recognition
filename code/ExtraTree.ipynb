{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Merge the text data\n",
    "vocab = pd.concat([train['comment_text'],  test['comment_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_word_dict={'ａ': 'a','！': ' !', '＋': ' +', '－': ' -', '．': ' .', '０': '0', \n",
    "                 '１': '1','２': '2', '３': '3', '４': '4', '５': '5', '６': '6', \n",
    "                 '７': '7', '８': '8', '９': '9', '＝': ' =', '？': ' ?', 'Ａ': 'a',\n",
    "                 'Ｂ': 'b','Ｃ': 'c', 'Ｄ': 'd', 'Ｅ': 'e', 'Ｆ': 'f', 'Ｇ': 'g', \n",
    "                 'Ｈ': 'h', 'Ｉ': 'i', 'Ｊ': 'j', 'Ｋ': 'k', 'Ｌ': 'l', 'Ｍ': 'm',\n",
    "                 'Ｎ': 'n', 'Ｏ': 'o', 'Ｐ': 'p', 'Ｑ': 'q', 'Ｒ': 'r', 'Ｓ': 's',\n",
    "                 'Ｔ': 't', 'Ｕ': 'u', 'Ｖ': 'v', 'Ｗ': 'w', 'Ｘ': 'x','Ｙ': 'y', \n",
    "                 'Ｚ': 'z', 'ｂ': 'b','ｃ': 'c', 'ｄ': 'd', 'ｅ': 'e', 'ｆ': 'f', \n",
    "                 'ｇ': 'g', 'ｈ': 'h', 'ｊ': 'j', 'ｋ': 'k', 'ｍ': 'm', 'ｎ': 'n', \n",
    "                 'ｏ': 'o', 'ｐ': 'p', 'ｑ': 'q', 'ｒ': 'r', 'ｓ': 's', 'ｔ': 't', \n",
    "                 'ｕ': 'u', 'ｖ': 'v', 'ｗ': 'w', 'ｘ': 'x', 'ｙ': 'y', 'ｚ': 'z'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def clean_dataset(word):\n",
    "    ##transfor all the uppercase letters to the lowercase letters\n",
    "    word = word.lower()\n",
    "    ##Turn unused information into spaces\n",
    "    word = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", word)\n",
    "    word = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", word)\n",
    "    for typo, correct in clean_word_dict.items():\n",
    "        word = re.sub(typo, \" \" + correct + \" \", word)\n",
    "    symbols = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "    word=symbols.sub(r' \\1 ', word)\n",
    "    return word\n",
    "\n",
    "train_comments = []\n",
    "test_comments = []\n",
    "for comment in train['comment_text']:\n",
    "    train_comments.append(clean_dataset(comment))\n",
    "    \n",
    "for comment in test['comment_text']:\n",
    "    test_comments.append(clean_dataset(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##do tfidf\n",
    "transform_function = TfidfVectorizer(\n",
    "    sublinear_tf=1,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=10000, ngram_range=(1, 2), strip_accents='unicode',\n",
       "                sublinear_tf=1, token_pattern='\\\\w{1,}')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Add data to model\n",
    "transform_function.fit(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train = transform_function.transform(train_comments)\n",
    "comments_test = transform_function.transform(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic 0.9490797066944688\n",
      "severe_toxic 0.9351889871049938\n",
      "obscene 0.9719671393144169\n",
      "threat 0.8609603094707186\n",
      "insult 0.9545448797702478\n",
      "identity_hate 0.8906286554177477\n"
     ]
    }
   ],
   "source": [
    "\n",
    "losses = []\n",
    "predictions = []\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = ExtraTreesClassifier(n_estimators=30) \n",
    "    score = np.mean(cross_val_score(classifier, comments_train, train_target, cv=2, scoring='roc_auc'))\n",
    "    print(class_name, score)  \n",
    "    classifier.fit(comments_train, train_target)\n",
    "    predictions.append(classifier.predict_proba(comments_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=pd.read_csv('test_labels.csv')\n",
    "labels=np.array(labels.iloc[:,1:])\n",
    "sum_labels=np.sum(labels,axis=1)\n",
    "idx=sum_labels>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 63978), (63978, 6))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_consider=np.array(predictions)[:,idx]\n",
    "labels_consider= labels[idx]\n",
    "preds_consider.shape,labels_consider.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9365694497823446"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores=[]\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for i in range(6):\n",
    "    scores.append(roc_auc_score(labels_consider[:,i],preds_consider[i,:]))\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
