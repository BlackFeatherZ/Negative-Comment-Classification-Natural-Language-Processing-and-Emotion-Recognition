{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W9pOCWgh8Sw7"
   },
   "outputs": [],
   "source": [
    "path=r'C:/Users/lenovo/Desktop/[DM] Group Project/Data-Mining-Project-master/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dP6ftkHZ9jCE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "FQiSVjyj9luT"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(path+\"train.csv\")\n",
    "test = pd.read_csv(path+\"test.csv\")\n",
    "labels = pd.read_csv(path+\"test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "C1PUU_z_9pCR",
    "outputId": "dd5de290-35b9-4ca3-80c2-86534dbbcd7e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  ... identity_hate\n",
       "0  0000997932d777bf  ...             0\n",
       "1  000103f0d9cfb60f  ...             0\n",
       "2  000113f07ec002fd  ...             0\n",
       "3  0001b41b1c6bb37e  ...             0\n",
       "4  0001d958c54c6e35  ...             0\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGUw11rI9q7S",
    "outputId": "6bd02d6a-ed6a-4a45-a8a5-22c8e61ab59f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(id               False\n",
       " comment_text     False\n",
       " toxic            False\n",
       " severe_toxic     False\n",
       " obscene          False\n",
       " threat           False\n",
       " insult           False\n",
       " identity_hate    False\n",
       " dtype: bool, id              False\n",
       " comment_text    False\n",
       " dtype: bool)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().any(),test.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NLlO7GHAFu6-"
   },
   "outputs": [],
   "source": [
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[classes].values\n",
    "list_sentences_train = train[\"comment_text\"]\n",
    "list_sentences_test = test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "s05dhgB-FiWX"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6baQPSdkFnEo"
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0pRrAJ7pFxmX"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UREu7LA_F4lU"
   },
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TMYlPdC4JzXL"
   },
   "outputs": [],
   "source": [
    "gl_path = '/content/drive/My Drive/Colab_data/Data_Mining/Data-Mining-Project/embeddings/glove.twitter.27B.25d.txt'\n",
    "ft_path = '/content/drive/My Drive/Colab_data/Data_Mining/Data-Mining-Project/embeddings/wiki.simple.vec'\n",
    "wv_path = '/content/drive/My Drive/Colab_data/Data_Mining/Data-Mining-Project/embeddings/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hZsMt6ihHS4i"
   },
   "outputs": [],
   "source": [
    "import gensim.models.keyedvectors as word2vec\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "jHtWe2E1F7BB"
   },
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix(typeToLoad):\n",
    "        #load different embedding file from Kaggle depending on which embedding \n",
    "        #matrix we are going to experiment with\n",
    "        if(typeToLoad==\"glove\"):\n",
    "            EMBEDDING_FILE=gl_path\n",
    "            embed_size = 25\n",
    "        elif(typeToLoad==\"word2vec\"):\n",
    "            word2vecDict = word2vec.KeyedVectors.load_word2vec_format(wv_path, binary=True)\n",
    "            embed_size = 300\n",
    "        elif(typeToLoad==\"fasttext\"):\n",
    "            EMBEDDING_FILE=ft_path\n",
    "            embed_size = 300\n",
    "\n",
    "        if(typeToLoad==\"glove\" or typeToLoad==\"fasttext\" ):\n",
    "            embeddings_index = dict()\n",
    "            #Transfer the embedding weights into a dictionary by iterating through every line of the file.\n",
    "            f = open(EMBEDDING_FILE)\n",
    "            for line in f:\n",
    "                #split up line into an indexed array\n",
    "                values = line.split()\n",
    "                #first index is word\n",
    "                word = values[0]\n",
    "                #store the rest of the values in the array as a new array\n",
    "                try:\n",
    "                  coefs = np.asarray(values[1:], dtype='float32')\n",
    "                except:\n",
    "                  continue\n",
    "                if len(coefs) != 300:\n",
    "                  continue\n",
    "                embeddings_index[word] = coefs #50 dimensions\n",
    "                \n",
    "            f.close()\n",
    "            print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "        else:\n",
    "            embeddings_index = dict()\n",
    "            for word in word2vecDict.wv.vocab:\n",
    "                embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "            print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            \n",
    "        gc.collect()\n",
    "        #We get the mean and standard deviation of the embedding weights so that we could maintain the \n",
    "        #same statistics for the rest of our own random generated weights. \n",
    "        all_embs = np.stack(list(embeddings_index.values()))\n",
    "        emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "        \n",
    "        nb_words = len(tokenizer.word_index)\n",
    "        #We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "        #the size will be Number of Words in Vocab X Embedding Size\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "        gc.collect()\n",
    "\n",
    "        #With the newly created embedding matrix, we'll fill it up with the words that we have in both \n",
    "        #our own dictionary and loaded pretrained embedding. \n",
    "        embeddedCount = 0\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            i-=1\n",
    "            #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            #and store inside the embedding matrix that we will train later on.\n",
    "            if embedding_vector is not None: \n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                embeddedCount+=1\n",
    "        print('total embedded:',embeddedCount,'common words')\n",
    "        \n",
    "        del(embeddings_index)\n",
    "        gc.collect()\n",
    "        \n",
    "        #finally, return the embedding matrix\n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_LCBxmSEW95",
    "outputId": "e6dd12e2-cee7-49e9-8eb1-4338fc5111ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 110995 word vectors.\n",
      "total embedded: 59312 common words\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = loadEmbeddingMatrix('fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irUlyn37Gp0W",
    "outputId": "1e8276f8-0219-4200-dff7-b8558b1be064"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221341, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "XxN-0Icgbp5h"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BcdmD1FnbiRl"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cakxu8f2b1yY",
    "outputId": "d90f0a8f-ec7d-40ba-e658-734cfb29effc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_layer will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_layer will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "x = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)\n",
    "x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "DZaoNGqdbr96"
   },
   "outputs": [],
   "source": [
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "PHhVSB7LelmI"
   },
   "outputs": [],
   "source": [
    "import keras.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "PoeXgv3ZcGaF"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[\n",
    "                          metrics.MeanSquaredError(),\n",
    "                          metrics.AUC(),\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PW7dB94TcHw_",
    "outputId": "68fa9351-adbb-4d9b-a950-2b74b20acecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 300)          66402300  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200, 120)          173280    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 66,581,936\n",
      "Trainable params: 179,636\n",
      "Non-trainable params: 66,402,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "xTcX5FYvmZHA"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_t,y,test_size=0.2,random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ysgA5zncJqG",
    "outputId": "72fa8bee-7c88-4605-e996-b12381a89c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "998/998 [==============================] - 1347s 1s/step - loss: 0.0667 - mean_squared_error: 0.0173 - auc: 0.9593\n",
      "Epoch 2/2\n",
      "998/998 [==============================] - 1315s 1s/step - loss: 0.0587 - mean_squared_error: 0.0156 - auc: 0.9697\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 2\n",
    "hist = model.fit(X_train,y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "IJdnXFKp1uO1"
   },
   "outputs": [],
   "source": [
    "preds_train = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcujEmttF9YV",
    "outputId": "466259f1-6844-40f0-f30e-d35043a51ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9749202734628901\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "\n",
    "print(roc_auc_score(y_train, preds_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVUhIPBGGQpv",
    "outputId": "5737c4e3-1f4c-459d-ddb4-b6a114d08c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9693278025347044\n"
     ]
    }
   ],
   "source": [
    "preds_val = model.predict(X_val)\n",
    "print(roc_auc_score(y_val, preds_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "VTEKRPU8GR_-"
   },
   "outputs": [],
   "source": [
    "labels = labels[classes]\n",
    "sum_labels=np.sum(labels.values,axis=1)\n",
    "# print(sum_labels)\n",
    "idx=sum_labels>=0\n",
    "y_test = labels[idx]\n",
    "X_test = X_te[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "dbNiGXHuGYpq"
   },
   "outputs": [],
   "source": [
    "preds_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojrhEz03GcM2",
    "outputId": "a4c6591c-a9b7-4fbd-931e-e8f01aa75bbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9597581026908394\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_test, preds_test))    #test accuracy for 'fasttext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEl9b6hZKfoB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "LSTM-fasttext.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
